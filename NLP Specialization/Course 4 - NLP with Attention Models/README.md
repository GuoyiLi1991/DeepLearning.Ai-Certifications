# Course 4: NLP with Attention Models

## Description
In Course 4 of the NLP Specialization, offered by DeepLearning.AI, you will: translate complete English sentences into German using an encoder-decoder attention model, build a Transformer model to summarize text, use T5 and BERT models to perform question-answering, and build a chatbot using a Reformer model.

- #### Week 1: Neural Machine Translation
	- Discover some of the shortcomings of a traditional seq2seq model and how to solve for them by adding an attention mechanism, then you will build a Neural Machine Translation model with Attention that translates English sentences into German.
- #### Week 2: Text Summarization 
	- Compare RNNs and other sequential models to the more modern Transformer architecture, then create a tool that generates text summaries.
- #### Week 3: Question Answering
	- Explore transfer learning with state-of-the-art models like T5 and BERT, then build a model that can answer questions.
- #### Week 4: Chatbot
	- Examine some unique challenges Transformer models face and their solutions, then build a chatbot using a Reformer model.

---

# Certification
<p align="center">
  <img src="../Natural Language Processing Certification Images/Courses/Natural_Language_Processing_with_Attention.jpg" | width=800 />
</p>
